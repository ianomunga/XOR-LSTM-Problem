{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XOR-LSTM.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPmsmCtguVQSrsohrJ5nwtM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ianomunga/XOR-LSTM-Problem/blob/main/XOR_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Solving the XOR Logic Gate Output Problem using an LSTM Recurrent Neural Network\n",
        "XOR stands for 'Exclusive-Or', which is a logical operator that evaluates to the 'True' Boolean output when either of its values are true; but not both. This mutual exclusivity is captured in this part of its name. 'exclusive'. \n",
        "\n",
        "This relationship is hard to represent in a linear way that a Logistic Regression Model would be able to generalize statistically for, because the statistical significance in the bits, i.e. the meaning, comes from a mutual rlationship between the two values under evaluation. \n",
        "\n",
        "A non-linearity could model this relationship, however, and that's where the LSTM Model comes in. It's 'Long Short-Term Memory' enables the cumulative evaluations of the stream of logic gates to be carried forward recurrently throughout the sequence.\n",
        "\n",
        "This is what the code below will implement.\n"
      ],
      "metadata": {
        "id": "Mxsrno4oUwpz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "szvVjUg1Ft-X"
      },
      "outputs": [],
      "source": [
        "#get all your dependencies in check\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.layers import Dense, Input, LSTM\n",
        "from tensorflow.keras.models import Sequential\n",
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#encapsulate some key variables, i.e\n",
        "\n",
        "#the sequence_length\n",
        "SEQ_LEN = 50\n",
        "\n",
        "#the number of bits in the sequence\n",
        "COUNT = 100000"
      ],
      "metadata": {
        "id": "tc9IxRA6GnUo"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create our pairs of logic gate values based on the cumulative sum of the generated sequence\n",
        "bin_pair = lambda x: [x, not(x)]\n",
        "training = np.array([[bin_pair(random.choice([0, 1])) for _ in range(SEQ_LEN)] for _ in range(COUNT)])\n",
        "target = np.array([[bin_pair(x) for x in np.cumsum(example[:,0]) % 2] for example in training])"
      ],
      "metadata": {
        "id": "wvRtUTokGvd_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check for a match between the lengths of the datasets before we go ahead\n",
        "print('shape check:', training.shape, '=', target.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSKczhmXG7B7",
        "outputId": "3aebc12a-669f-48c4-94b9-4942cd19d0f2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape check: (100000, 50, 2) = (100000, 50, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "#pass in the sequence-length so that every possible example's dimension is accounted for \n",
        "model.add(Input(shape=(SEQ_LEN, 2), dtype='float32'))\n",
        "#build the model with the LSTM component for parity persistence,\n",
        "model.add(LSTM(1, return_sequences=True))\n",
        "#two possible outcomes for the two possible logicgate values\n",
        "model.add(Dense(2, activation='softmax'))"
      ],
      "metadata": {
        "id": "yJZnOmTJHIkb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#now fit the model to the data and run the epochs\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(training, target, epochs=10, batch_size=128)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gp6Z1ZtyHT4d",
        "outputId": "08561dbb-8849-42bb-da04-e74156ffb1d6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "782/782 [==============================] - 19s 21ms/step - loss: 0.6934 - accuracy: 0.4946\n",
            "Epoch 2/10\n",
            "782/782 [==============================] - 16s 21ms/step - loss: 0.6931 - accuracy: 0.5012\n",
            "Epoch 3/10\n",
            "782/782 [==============================] - 16s 21ms/step - loss: 0.6907 - accuracy: 0.5148\n",
            "Epoch 4/10\n",
            "782/782 [==============================] - 16s 21ms/step - loss: 0.4587 - accuracy: 0.8355\n",
            "Epoch 5/10\n",
            "782/782 [==============================] - 16s 21ms/step - loss: 0.1898 - accuracy: 1.0000\n",
            "Epoch 6/10\n",
            "782/782 [==============================] - 17s 21ms/step - loss: 0.1300 - accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "782/782 [==============================] - 17s 21ms/step - loss: 0.0964 - accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "782/782 [==============================] - 17s 21ms/step - loss: 0.0740 - accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "782/782 [==============================] - 17s 21ms/step - loss: 0.0578 - accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "782/782 [==============================] - 17s 21ms/step - loss: 0.0455 - accuracy: 1.0000\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_1 (LSTM)               (None, 50, 1)             16        \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 50, 2)             4         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20\n",
            "Trainable params: 20\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(training)\n",
        "i = random.randint(0, COUNT)\n",
        "chance = predictions[i,-1,0]\n",
        "print('randomly selected sequence:', training[i,:,0])\n",
        "print('prediction:', int(chance > 0.5))\n",
        "print('confidence: {:0.2f}%'.format((chance if chance > 0.5 else 1 - chance) * 100))\n",
        "print('actual:', np.sum(training[i,:,0]) % 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsV8th8nHb4e",
        "outputId": "9222a0da-2832-4fb5-f602-fda3217c5b23"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "randomly selected sequence: [1 1 0 1 0 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 0 0 1 0 1 0\n",
            " 0 0 0 0 1 1 0 0 0 1 1 0 1]\n",
            "prediction: 1\n",
            "confidence: 99.73%\n",
            "actual: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It can be seen that the LSTM configuration does successfully carry forward the parity of the logical gates. In the end, the model is able to predict the parity of the alternatives to a sequence of randomly generated bits with a confidence score of 99.73 percent with 100,000 sample bits serving as the examples in 50-bit sequences. "
      ],
      "metadata": {
        "id": "skpkDbLne005"
      }
    }
  ]
}